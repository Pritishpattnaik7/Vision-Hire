{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37f5c8ed-f21f-40a6-b50b-8f1289dd72b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting calibration. Please be in neutral state\n",
      "Starting main application\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'minThreshold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 588>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarting main application\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    587\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 588\u001b[0m \u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mears_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmars_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpucs_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmoes_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    590\u001b[0m face_mesh\u001b[38;5;241m.\u001b[39mclose()\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36minfer\u001b[1;34m(ears_norm, mars_norm, pucs_norm, moes_norm)\u001b[0m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIgnoring empty camera frame.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 484\u001b[0m \u001b[43mcheck2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[0;32m    485\u001b[0m count\u001b[38;5;241m=\u001b[39mcheck(image)\n\u001b[0;32m    487\u001b[0m ear, mar, puc, moe, image,look \u001b[38;5;241m=\u001b[39m run_face_mp(image)\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mcheck2\u001b[1;34m(im)\u001b[0m\n\u001b[0;32m     47\u001b[0m      confstr \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m100\u001b[39m \u001b[38;5;241m-\u001b[39m conf))         \n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# if (100-conf) > 67:\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m  \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m-\u001b[39mconf) \u001b[38;5;241m>\u001b[39m \u001b[43mminThreshold\u001b[49m:\n\u001b[0;32m     50\u001b[0m      ts \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     51\u001b[0m      timeStamp \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mfromtimestamp(ts)\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'minThreshold' is not defined"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from csv import writer \n",
    "import cv2 \n",
    "import mediapipe as mp\n",
    "import math\n",
    "import os \n",
    "#import mtcnn\n",
    "import time\n",
    "import torch\n",
    "import cv2\n",
    "import imutils\n",
    "#from deepface import DeepFace\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "face_detection = mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5)\n",
    "flag=1\n",
    "unique_faces = {}\n",
    "LEFT_EYE =[ 362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385,384, 398 ]\n",
    "LEFT_EYEBROW =[ 336, 296, 334, 293, 300, 276, 283, 282, 295, 285 ]\n",
    "#face_detector = mtcnn.MTCNN()\n",
    "\n",
    "minThreshold = 40\n",
    "\n",
    "\n",
    "def check2(im):\n",
    "    recognizer = cv2.face.LBPHFaceRecognizer_create()   # cv2.createLBPHFaceRecognizer()\n",
    "    recognizer.read(\"TrainingImageLabel\"+os.sep+\"Trainner.yml\")\n",
    "    harcascadePath = \"haarcascade_frontalface_default.xml\"\n",
    "    faceCascade = cv2.CascadeClassifier(harcascadePath)\n",
    "    df = pd.read_csv(\"EmployeeDetails\"+os.sep+\"EmployeeDetails.csv\")\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    col_names = ['Id', 'Name', 'Time']\n",
    "    attendance = pd.DataFrame(columns=col_names)\n",
    "    gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "    faces = faceCascade.detectMultiScale(gray, 1.2, 5,flags = cv2.CASCADE_SCALE_IMAGE)\n",
    "    for(x, y, w, h) in faces:\n",
    "        cv2.rectangle(im, (x, y), (x+w, y+h), (10, 159, 255), 2)\n",
    "        Id, conf = recognizer.predict(gray[y:y+h, x:x+w])\n",
    "\n",
    "        if conf < 100:\n",
    "            aa = df.loc[df['Id'] == Id]['Name'].values\n",
    "            confstr = \"  {0}%\".format(round(100 - conf))\n",
    "            tt = str(Id)+\"-\"+aa\n",
    "        else:\n",
    "            Id = '  Unknown  '\n",
    "            tt = str(Id)\n",
    "            confstr = \"  {0}%\".format(round(100 - conf))         \n",
    "       # if (100-conf) > 67:\n",
    "        if (100-conf) > minThreshold:\n",
    "            ts = time.time()\n",
    "            timeStamp = datetime.datetime.fromtimestamp(ts).strftime('%H:%M:%S')\n",
    "            aa = str(aa)[2:-2]\n",
    "            attendance.loc[len(attendance)] = [Id, aa, timeStamp]\n",
    "            tt = str(tt)[2:-2]\n",
    "        if(100-conf) > minThreshold:\n",
    "            tt = tt + \" [Pass]\"\n",
    "            cv2.putText(im, str(tt), (x+5,y-5), font, 1, (255, 255, 255), 2)\n",
    "        else:\n",
    "            cv2.putText(im, str(tt), (x + 5, y - 5), font, 1, (255, 255, 255), 2)\n",
    "\n",
    "        if (100-conf) > minThreshold:\n",
    "            cv2.putText(im, str(confstr), (x + 5, y + h - 5), font,1, (0, 255, 0),1 )\n",
    "        elif (100-conf) > 50:\n",
    "            cv2.putText(im, str(confstr), (x + 5, y + h - 5), font, 1, (0, 255, 255), 1)\n",
    "        else:\n",
    "            cv2.putText(im, str(confstr), (x + 5, y + h - 5), font, 1, (0, 0, 255), 1)\n",
    "    \n",
    "\n",
    "# def detect(image):\n",
    "#     frame = imutils.resize(image, width=800)\n",
    "#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     faces = face_detector.detect_faces(frame)\n",
    "#     for coordinates in faces:                     # Loop over the detected faces #for (x, y, w, h) in faces:\n",
    "#           x, y, w, h = coordinates['box']          # Crop the face from the frame\n",
    "#           face = frame[y:y+h, x:x+w]\n",
    "#           unique_faces[0] = {'fac': face}\n",
    "# def verify(image):\n",
    "#     frame = imutils.resize(image, width=800)\n",
    "#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     faces = face_detector.detect_faces(frame)\n",
    "    \n",
    "#     for coordinates in faces:                     # Loop over the detected faces #for (x, y, w, h) in faces:\n",
    "#           x, y, w, h = coordinates['box']          # Crop the face from the frame\n",
    "#           face = frame[y:y+h, x:x+w]\n",
    "#           if DeepFace.verify(face,unique_faces[0]['fac'],enforce_detection = False)['verified']:\n",
    "#             return True\n",
    "#     return False\n",
    "    \n",
    "# right eyes indices\n",
    "RIGHT_EYE=[ 33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161 , 246 ]  \n",
    "RIGHT_EYEBROW=[ 70, 63, 105, 66, 107, 55, 65, 52, 53, 46 ]\n",
    "def landmarksDetection(img, results, draw=False):\n",
    "    img_height, img_width= img.shape[:2]\n",
    "    # list[(x,y), (x,y)....]\n",
    "    mesh_coord = [(int(point.x * img_width), int(point.y * img_height)) for point in results.multi_face_landmarks[0].landmark]\n",
    "    if draw :\n",
    "        [cv2.circle(img, p, 2, (0,255,0), -1) for p in mesh_coord]\n",
    "\n",
    "    # returning the list of tuples for each landmarks \n",
    "    return mesh_coord\n",
    "def eyesExtractor(img, right_eye_coords, left_eye_coords):\n",
    "    # converting color image to  scale image \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # getting the dimension of image \n",
    "    dim = gray.shape\n",
    "\n",
    "    # creating mask from gray scale dim\n",
    "    mask = np.zeros(dim, dtype=np.uint8)\n",
    "    \n",
    "    # drawing Eyes Shape on mask with white color \n",
    "    cv2.fillPoly(mask, [np.array(right_eye_coords, dtype=np.int32)], 255)\n",
    "    cv2.fillPoly(mask, [np.array(left_eye_coords, dtype=np.int32)], 255)\n",
    "\n",
    "    # showing the mask \n",
    "    # cv2.imshow('mask', mask)\n",
    "    \n",
    "    # draw eyes image on mask, where white shape is \n",
    "    eyes = cv2.bitwise_and(gray, gray, mask=mask)\n",
    "    # change black color to gray other than eys \n",
    "    # cv2.imshow('eyes draw', eyes)\n",
    "    eyes[mask==0]=155\n",
    "    \n",
    "    # getting minium and maximum x and y  for right and left eyes \n",
    "    # For Right Eye \n",
    "    r_max_x = (max(right_eye_coords, key=lambda item: item[0]))[0]\n",
    "    r_min_x = (min(right_eye_coords, key=lambda item: item[0]))[0]\n",
    "    r_max_y = (max(right_eye_coords, key=lambda item : item[1]))[1]\n",
    "    r_min_y = (min(right_eye_coords, key=lambda item: item[1]))[1]\n",
    "\n",
    "    # For LEFT Eye\n",
    "    l_max_x = (max(left_eye_coords, key=lambda item: item[0]))[0]\n",
    "    l_min_x = (min(left_eye_coords, key=lambda item: item[0]))[0]\n",
    "    l_max_y = (max(left_eye_coords, key=lambda item : item[1]))[1]\n",
    "    l_min_y = (min(left_eye_coords, key=lambda item: item[1]))[1]\n",
    "\n",
    "    # croping the eyes from mask \n",
    "    cropped_right = eyes[r_min_y: r_max_y, r_min_x: r_max_x]\n",
    "    cropped_left = eyes[l_min_y: l_max_y, l_min_x: l_max_x]\n",
    "\n",
    "    # returning the cropped eyes \n",
    "    return cropped_right, cropped_left\n",
    "\n",
    "# Eyes Postion Estimator \n",
    "def positionEstimator(cropped_eye):\n",
    "    # getting height and width of eye \n",
    "    h, w =cropped_eye.shape\n",
    "    \n",
    "    # remove the noise from images\n",
    "    gaussain_blur = cv2.GaussianBlur(cropped_eye, (9,9),0)\n",
    "    median_blur = cv2.medianBlur(gaussain_blur, 3)\n",
    "\n",
    "    # applying thrsholding to convert binary_image\n",
    "    ret, threshed_eye = cv2.threshold(median_blur, 130, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # create fixd part for eye with \n",
    "    piece = int(w/3) \n",
    "\n",
    "    # slicing the eyes into three parts \n",
    "    right_piece = threshed_eye[0:h, 0:piece]\n",
    "    center_piece = threshed_eye[0:h, piece: piece+piece]\n",
    "    left_piece = threshed_eye[0:h, piece +piece:w]\n",
    "    \n",
    "    # calling pixel counter function\n",
    "    eye_position = pixelCounter(right_piece, center_piece, left_piece)\n",
    "\n",
    "    return eye_position \n",
    "\n",
    "# creating pixel counter function \n",
    "def pixelCounter(first_piece, second_piece, third_piece):\n",
    "    # counting black pixel in each part \n",
    "    right_part = np.sum(first_piece==0)\n",
    "    center_part = np.sum(second_piece==0)\n",
    "    left_part = np.sum(third_piece==0)\n",
    "    # creating list of these values\n",
    "    eye_parts = [right_part, center_part, left_part]\n",
    "\n",
    "    # getting the index of max values in the list \n",
    "    max_index = eye_parts.index(max(eye_parts))\n",
    "    pos_eye ='' \n",
    "    if max_index==0:\n",
    "        pos_eye=\"RIGHT\"\n",
    "    elif max_index==1:\n",
    "        pos_eye = 'CENTER'\n",
    "    elif max_index ==2:\n",
    "        pos_eye = 'LEFT'\n",
    "    else:\n",
    "        pos_eye=\"Closed\"\n",
    "    return pos_eye\n",
    "def distance(p1, p2):\n",
    "    ''' Calculate distance between two points\n",
    "    :param p1: First Point \n",
    "    :param p2: Second Point\n",
    "    :return: Euclidean distance between the points. (Using only the x and y coordinates).\n",
    "    '''\n",
    "    return (((p1[:2] - p2[:2])**2).sum())**0.5\n",
    "\n",
    "\n",
    "def check(image):\n",
    "    mp_face_detection = mp.solutions.face_detection\n",
    "\n",
    "    face_detection = mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.3)\n",
    "    face_detection_results = face_detection.process(image)\n",
    "    count=0\n",
    "    if face_detection_results.detections:\n",
    "        count= len(face_detection_results.detections)\n",
    "        return count\n",
    "\n",
    "\n",
    "def eye_aspect_ratio(landmarks, eye):\n",
    "    ''' Calculate the ratio of the eye length to eye width. \n",
    "    :param landmarks: Face Landmarks returned from FaceMesh MediaPipe model\n",
    "    :param eye: List containing positions which correspond to the eye\n",
    "    :return: Eye aspect ratio value\n",
    "    '''\n",
    "    N1 = distance(landmarks[eye[1][0]], landmarks[eye[1][1]])\n",
    "    N2 = distance(landmarks[eye[2][0]], landmarks[eye[2][1]])\n",
    "    N3 = distance(landmarks[eye[3][0]], landmarks[eye[3][1]])\n",
    "    D = distance(landmarks[eye[0][0]], landmarks[eye[0][1]])\n",
    "    return (N1 + N2 + N3) / (3 * D)\n",
    "\n",
    "def eye_feature(landmarks):\n",
    "    ''' Calculate the eye feature as the average of the eye aspect ratio for the two eyes\n",
    "    :param landmarks: Face Landmarks returned from FaceMesh MediaPipe model\n",
    "    :return: Eye feature value\n",
    "    '''\n",
    "    return (eye_aspect_ratio(landmarks, left_eye) + \\\n",
    "    eye_aspect_ratio(landmarks, right_eye))/2\n",
    "\n",
    "def mouth_feature(landmarks):\n",
    "    ''' Calculate mouth feature as the ratio of the mouth length to mouth width\n",
    "    :param landmarks: Face Landmarks returned from FaceMesh MediaPipe model\n",
    "    :return: Mouth feature value\n",
    "    '''\n",
    "    N1 = distance(landmarks[mouth[1][0]], landmarks[mouth[1][1]])\n",
    "    N2 = distance(landmarks[mouth[2][0]], landmarks[mouth[2][1]])\n",
    "    N3 = distance(landmarks[mouth[3][0]], landmarks[mouth[3][1]])\n",
    "    D = distance(landmarks[mouth[0][0]], landmarks[mouth[0][1]])\n",
    "    return (N1 + N2 + N3)/(3*D)\n",
    "\n",
    "def pupil_circularity(landmarks, eye):\n",
    "    ''' Calculate pupil circularity feature.\n",
    "    :param landmarks: Face Landmarks returned from FaceMesh MediaPipe model\n",
    "    :param eye: List containing positions which correspond to the eye\n",
    "    :return: Pupil circularity for the eye coordinates\n",
    "    '''\n",
    "    perimeter = distance(landmarks[eye[0][0]], landmarks[eye[1][0]]) + \\\n",
    "            distance(landmarks[eye[1][0]], landmarks[eye[2][0]]) + \\\n",
    "            distance(landmarks[eye[2][0]], landmarks[eye[3][0]]) + \\\n",
    "            distance(landmarks[eye[3][0]], landmarks[eye[0][1]]) + \\\n",
    "            distance(landmarks[eye[0][1]], landmarks[eye[3][1]]) + \\\n",
    "            distance(landmarks[eye[3][1]], landmarks[eye[2][1]]) + \\\n",
    "            distance(landmarks[eye[2][1]], landmarks[eye[1][1]]) + \\\n",
    "            distance(landmarks[eye[1][1]], landmarks[eye[0][0]])\n",
    "    area = math.pi * ((distance(landmarks[eye[1][0]], landmarks[eye[3][1]]) * 0.5) ** 2)\n",
    "    return (4*math.pi*area)/(perimeter**2)\n",
    "\n",
    "def pupil_feature(landmarks):\n",
    "    ''' Calculate the pupil feature as the average of the pupil circularity for the two eyes\n",
    "    :param landmarks: Face Landmarks returned from FaceMesh MediaPipe model\n",
    "    :return: Pupil feature value\n",
    "    '''\n",
    "    return (pupil_circularity(landmarks, left_eye) + \\\n",
    "        pupil_circularity(landmarks, right_eye))/2\n",
    "\n",
    "def run_face_mp1(image):\n",
    "    ''' Get face landmarks using the FaceMesh MediaPipe model. \n",
    "    Calculate facial features using the landmarks.\n",
    "    :param image: Image for which to get the face landmarks\n",
    "    :return: Feature 1 (Eye), Feature 2 (Mouth), Feature 3 (Pupil), \\\n",
    "        Feature 4 (Combined eye and mouth feature), image with mesh drawings\n",
    "    '''\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = face_mesh.process(image)\n",
    "    \n",
    "    \n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    if results.multi_face_landmarks:\n",
    "        for faceLms in results.multi_face_landmarks:\n",
    "            #mp_drawing.draw_landmarks(image, faceLms, mp_face_mesh.FACEMESH_CONTOURS)\n",
    "            #my change\n",
    "            \n",
    "           \n",
    "            \n",
    "            h, w, c = image.shape\n",
    "            cx_min=  w\n",
    "            cy_min = h\n",
    "            cx_max= cy_max= 0\n",
    "            for id, lm in enumerate(faceLms.landmark):\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                if cx<cx_min:\n",
    "                    cx_min=cx\n",
    "                if cy<cy_min:\n",
    "                    cy_min=cy\n",
    "                if cx>cx_max:\n",
    "                    cx_max=cx\n",
    "                if cy>cy_max:\n",
    "                    cy_max=cy\n",
    "            cv2.rectangle(image, (cx_min, cy_min), (cx_max, cy_max), (255, 255, 0), 2)\n",
    "        landmarks_positions = []\n",
    "        # assume that only face is present in the image\n",
    "        for _, data_point in enumerate(results.multi_face_landmarks[0].landmark):\n",
    "            landmarks_positions.append([data_point.x, data_point.y, data_point.z]) # saving normalized landmark positions\n",
    "        landmarks_positions = np.array(landmarks_positions)\n",
    "        landmarks_positions[:, 0] *= image.shape[1]\n",
    "        landmarks_positions[:, 1] *= image.shape[0]\n",
    "\n",
    "        # draw face mesh over image\n",
    "        # for face_landmarks in results.multi_face_landmarks:\n",
    "        #             mp_drawing.draw_landmarks(\n",
    "        #             image=image,\n",
    "        #             landmark_list=face_landmarks,\n",
    "        #             connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "        #             landmark_drawing_spec=drawing_spec,\n",
    "        #             connection_drawing_spec=drawing_spec)\n",
    "\n",
    "        ear = eye_feature(landmarks_positions)\n",
    "        mar = mouth_feature(landmarks_positions)\n",
    "        puc = pupil_feature(landmarks_positions)\n",
    "        moe = mar/ear\n",
    "    else:\n",
    "        ear = -1000\n",
    "        mar = -1000\n",
    "        puc = -1000\n",
    "        moe = -1000\n",
    "\n",
    "    return ear, mar, puc, moe, image\n",
    "\n",
    "def run_face_mp(image):\n",
    "    ''' Get face landmarks using the FaceMesh MediaPipe model. \n",
    "    Calculate facial features using the landmarks.\n",
    "    :param image: Image for which to get the face landmarks\n",
    "    :return: Feature 1 (Eye), Feature 2 (Mouth), Feature 3 (Pupil), \\\n",
    "        Feature 4 (Combined eye and mouth feature), image with mesh drawings\n",
    "    '''\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = face_mesh.process(image)\n",
    "    \n",
    "\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    if results.multi_face_landmarks:\n",
    "        mesh_coords = landmarksDetection(image, results, False)\n",
    "        right_coords = [mesh_coords[p] for p in RIGHT_EYE]\n",
    "        left_coords = [mesh_coords[p] for p in LEFT_EYE]\n",
    "        crop_right, crop_left = eyesExtractor(image, right_coords, left_coords)\n",
    "        # cv.imshow('right', crop_right)\n",
    "        # cv.imshow('left', crop_left)\n",
    "        eye_position = positionEstimator(crop_right)\n",
    "        \n",
    "        for faceLms in results.multi_face_landmarks:\n",
    "            #mp_drawing.draw_landmarks(image, faceLms, mp_face_mesh.FACEMESH_CONTOURS)\n",
    "            #my change\n",
    "            \n",
    "            h, w, c = image.shape\n",
    "            cx_min=  w\n",
    "            cy_min = h\n",
    "            cx_max= cy_max= 0\n",
    "            for id, lm in enumerate(faceLms.landmark):\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                if cx<cx_min:\n",
    "                    cx_min=cx\n",
    "                if cy<cy_min:\n",
    "                    cy_min=cy\n",
    "                if cx>cx_max:\n",
    "                    cx_max=cx\n",
    "                if cy>cy_max:\n",
    "                    cy_max=cy\n",
    "            cv2.rectangle(image, (cx_min, cy_min), (cx_max, cy_max), (255, 255, 0), 2)\n",
    "        landmarks_positions = []\n",
    "        # assume that only face is present in the image\n",
    "        for _, data_point in enumerate(results.multi_face_landmarks[0].landmark):\n",
    "            landmarks_positions.append([data_point.x, data_point.y, data_point.z]) # saving normalized landmark positions\n",
    "        landmarks_positions = np.array(landmarks_positions)\n",
    "        landmarks_positions[:, 0] *= image.shape[1]\n",
    "        landmarks_positions[:, 1] *= image.shape[0]\n",
    "\n",
    "        # draw face mesh over image\n",
    "        # for face_landmarks in results.multi_face_landmarks:\n",
    "        #             mp_drawing.draw_landmarks(\n",
    "        #             image=image,\n",
    "        #             landmark_list=face_landmarks,\n",
    "        #             connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "        #             landmark_drawing_spec=drawing_spec,\n",
    "        #             connection_drawing_spec=drawing_spec)\n",
    "\n",
    "        ear = eye_feature(landmarks_positions)\n",
    "        mar = mouth_feature(landmarks_positions)\n",
    "        puc = pupil_feature(landmarks_positions)\n",
    "        moe = mar/ear\n",
    "    else:\n",
    "        ear = -1000\n",
    "        mar = -1000\n",
    "        puc = -1000\n",
    "        moe = -1000\n",
    "\n",
    "    return ear, mar, puc, moe, image,eye_position\n",
    "\n",
    "def calibrate(calib_frame_count=25):\n",
    "    ''' Perform clibration. Get features for the neutral position.\n",
    "    :param calib_frame_count: Image frames for which calibration is performed. Default Vale of 25.\n",
    "    :return: Normalization Values for feature 1, Normalization Values for feature 2, \\\n",
    "        Normalization Values for feature 3, Normalization Values for feature 4\n",
    "    '''\n",
    "    ears = []\n",
    "    mars = []\n",
    "    pucs = []\n",
    "    moes = []\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "        #detect(image)\n",
    "        ear, mar,puc, moe, image= run_face_mp1(image)\n",
    "        if ear != -1000:\n",
    "            ears.append(ear)\n",
    "            mars.append(mar)\n",
    "            pucs.append(puc)\n",
    "            moes.append(moe)\n",
    "\n",
    "        cv2.putText(image, \"Calibration\", (int(0.02*image.shape[1]), int(0.14*image.shape[0])),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 0, 0), 2)\n",
    "        cv2.imshow('MediaPipe FaceMesh', image)\n",
    "        if cv2.waitKey(5) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "        if len(ears) >= calib_frame_count:\n",
    "            break\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()\n",
    "    ears = np.array(ears)\n",
    "    mars = np.array(mars)\n",
    "    pucs = np.array(pucs)\n",
    "    moes = np.array(moes)\n",
    "    return [ears.mean(), ears.std()], [mars.mean(), mars.std()], \\\n",
    "        [pucs.mean(), pucs.std()], [moes.mean(), moes.std()]\n",
    "\n",
    "def get_classification(input_data):\n",
    "    ''' Perform classification over the facial  features.\n",
    "    :param input_data: List of facial features for 20 frames\n",
    "    :return: Alert / Drowsy state prediction\n",
    "    '''\n",
    "    model_input = []\n",
    "    model_input.append(input_data[:5])\n",
    "    model_input.append(input_data[3:8])\n",
    "    model_input.append(input_data[6:11])\n",
    "    model_input.append(input_data[9:14])\n",
    "    model_input.append(input_data[12:17])\n",
    "    model_input.append(input_data[15:])\n",
    "    model_input = torch.FloatTensor(np.array(model_input))\n",
    "    preds = torch.sigmoid(model(model_input)).gt(0.5).int().data.numpy()\n",
    "    return int(preds.sum() >= 5)\n",
    "\n",
    "def infer(ears_norm, mars_norm, pucs_norm, moes_norm):\n",
    "    ''' Perform inference.\n",
    "    :param ears_norm: Normalization values for eye feature\n",
    "    :param mars_norm: Normalization values for mouth feature\n",
    "    :param pucs_norm: Normalization values for pupil feature\n",
    "    :param moes_norm: Normalization values for mouth over eye feature. \n",
    "    '''\n",
    "    i=0\n",
    "    ear_main = 0\n",
    "    mar_main = 0\n",
    "    puc_main = 0\n",
    "    moe_main = 0\n",
    "    decay = 0.9 # use decay to smoothen the noise in feature values\n",
    "\n",
    "    label = None\n",
    "    input_data = []\n",
    "    frame_before_run = 0\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "        check2(image)    \n",
    "        count=check(image)\n",
    "        \n",
    "        ear, mar, puc, moe, image,look = run_face_mp(image)\n",
    "        mp_drawing = mp.solutions.drawing_utils \n",
    "        drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "        if ear != -1000:\n",
    "            ear = (ear - ears_norm[0])/ears_norm[1]\n",
    "            mar = (mar - mars_norm[0])/mars_norm[1]\n",
    "            puc = (puc - pucs_norm[0])/pucs_norm[1]\n",
    "            moe = (moe - moes_norm[0])/moes_norm[1]\n",
    "            if ear_main == -1000:\n",
    "                ear_main = ear\n",
    "                mar_main = mar\n",
    "                puc_main = puc\n",
    "                moe_main = moe\n",
    "            else:\n",
    "                ear_main = ear_main*decay + (1-decay)*ear\n",
    "                mar_main = mar_main*decay + (1-decay)*mar\n",
    "                puc_main = puc_main*decay + (1-decay)*puc\n",
    "                moe_main = moe_main*decay + (1-decay)*moe\n",
    "        else:\n",
    "            ear_main = -1000\n",
    "            mar_main = -1000\n",
    "            puc_main = -1000\n",
    "            moe_main = -1000\n",
    "        \n",
    "        if len(input_data) == 20:\n",
    "            input_data.pop(0)\n",
    "        input_data.append([ear_main, mar_main, puc_main, moe_main])\n",
    "\n",
    "        frame_before_run += 1\n",
    "        if frame_before_run >= 15 and len(input_data) == 20:\n",
    "            frame_before_run = 0\n",
    "            label = get_classification(input_data)\n",
    "            print ('got label ', label)\n",
    "        \n",
    "        cv2.putText(image, \"EAR: %.2f\" %(ear_main), (int(0.02*image.shape[1]), int(0.07*image.shape[0])),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "        cv2.putText(image, \"MAR: %.2f\" %(mar_main), (int(0.27*image.shape[1]), int(0.07*image.shape[0])),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "        cv2.putText(image, \"PUC: %.2f\" %(puc_main), (int(0.52*image.shape[1]), int(0.07*image.shape[0])),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "        cv2.putText(image, \"MOE: %.2f\" %(moe_main), (int(0.77*image.shape[1]), int(0.07*image.shape[0])),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "        cv2.putText(image,look, (int(0.77*image.shape[1]), int(0.2*image.shape[0])),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "        \n",
    "        if count>1:\n",
    "\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "            # org\n",
    "            org = (50, 150)\n",
    "\n",
    "            # fontScale\n",
    "            fontScale = 1\n",
    "\n",
    "            # Blue color in BGR\n",
    "            color = (255, 0, 0)\n",
    "\n",
    "            # Line thickness of 2 px\n",
    "            thickness = 2\n",
    "\n",
    "            # Using cv2.putText() method\n",
    "            image = cv2.putText(image, 'FRAUD', org, font, \n",
    "                               fontScale, color, thickness, cv2.LINE_AA)\n",
    "        if label is not None:\n",
    "            if label == 0:\n",
    "                color = (0, 255, 0)\n",
    "            else:\n",
    "                color = (0, 0, 255)\n",
    "            cv2.putText(image, \"%s\" %(states[label]), (int(0.02*image.shape[1]), int(0.2*image.shape[0])),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.5, color, 2)\n",
    "\n",
    "        cv2.imshow('MediaPipe FaceMesh', image)\n",
    "        if cv2.waitKey(5) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()\n",
    "\n",
    "\n",
    "right_eye = [[33, 133], [160, 144], [159, 145], [158, 153]] # right eye landmark positions\n",
    "left_eye = [[263, 362], [387, 373], [386, 374], [385, 380]] # left eye landmark positions\n",
    "mouth = [[61, 291], [39, 181], [0, 17], [269, 405]] # mouth landmark coordinates\n",
    "states = ['alert', 'drowsy']\n",
    "\n",
    "# Declaring FaceMesh model\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    min_detection_confidence=0.3, min_tracking_confidence=0.8)\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "model_lstm_path = 'models\\clf_lstm_jit6.pth'\n",
    "model = torch.jit.load(model_lstm_path)\n",
    "model.eval()\n",
    "\n",
    "print ('Starting calibration. Please be in neutral state')\n",
    "time.sleep(1)\n",
    "ears_norm, mars_norm, pucs_norm, moes_norm = calibrate()\n",
    "\n",
    "print ('Starting main application')\n",
    "time.sleep(1)\n",
    "infer(ears_norm, mars_norm, pucs_norm, moes_norm)\n",
    "\n",
    "face_mesh.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9b830f-411d-48e7-b1aa-7d75554d1daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32300e79-5575-4009-b8e3-036acc36aa61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d10efbc-2922-4658-bd8a-16c84a5bc696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
